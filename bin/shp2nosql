#!/bin/bash

# get the base directory of the package
script_dir="$( cd "$( dirname `which shp2nosql` )" && cd ../ && pwd )"

# preallocate variables
is_local=false
multiple_files=false
host=localhost
remove=false
use_esbulk=false

# with the exception of the documentation, these functions set
# variables and display warnings only

h_opt () {
    cat "$script_dir"/help.txt
}

# check if shapefile is local or should be downloaded
l_opt () {
    is_local=true
}

# use this option if using multiple files (no need to specify -l as well)
m_opt () {
    multiple_files=true
    is_local=true
}

# download data from tiger or use local file
f_opt () {
    if [ "$is_local" = true ]
    then
        if [ "$multiple_files" = true ]
        then
            shapefile_dir="$OPTARG"
            shapefile_dir="$(realpath $shapefile_dir)" # this allows input to be relative path to file
            # get number of files in directory; pipe ls output to grep (shp$ means no .shp.xml, .shp.iso.xml, etc.) then count lines
            num_files=$(ls "$shapefile_dir" | grep shp$ | wc -l)
            echo "Directory of shapfiles is $shapefile_dir"
            echo "$num_files files present"
            #exit 0
        else
            shapefile="$OPTARG"
            shapefile="$(realpath $shapefile)" # this allows input to be relative path to file
            if [ -a "$shapefile" ] # check if shapefile exists
            then
                echo "Using shapefile $shapefile"
            else
                echo "File does not exist"
                exit 1
            fi
        fi
    else
        # "${OPTARG,,}" converts the argument to lowercase via bash string manipulation
        census_prod="${OPTARG,,}"
        if [ $census_prod != "state" ]  && \
               [ $census_prod != "county" ] && \
               [ $census_prod != "tract" ]
        then
            echo "Census retrieval must be either state, county, or tract" >&2
            exit 1
        fi
    fi
}

# get state fips code
s_opt () {
    state_fips="$OPTARG" # should be numeric, no need to convert to lower
    num_digits="${#state_fips}"
    if [ "$num_digits" -eq 2 ]
    then
        echo "Using state fips code $state_fips"
    elif [ "$num_digits" -eq 1 ]
    then
        state_fips=0"$state_fips"
        echo "Using state fips code $state_fips"
    else
        echo "State fips code must be a two digit integer"
    fi
}

# get database type
d_opt () {
    db_type="${OPTARG,,}"
    if [ "$db_type" = "elasticsearch" ]
    then
        # TODO what if this argument comes after the port argument?
        port=9200
        echo "Using database type $db_type"
    elif
        [ "$db_type" = "mongodb" ]
    then
        port=27017
        echo "Using database type $db_type"
    else
        echo "Databse type must be either 'elasticsearch' or 'mongodb'" >&2
        exit 1
    fi
}

# get database name (MongoDB only)
D_opt () {
    db_name="$OPTARG" # should not be converted to lowercase; document types can be upper or lower
    echo "Using database $db_name"
}

# Get Collection name (MongoDB only)
c_opt () {
    collection_name="$OPTARG"
    echo "Using collection $collection_name"
}

# get host (external ip_address)
H_opt () {
    host="$OPTARG"
    echo "Using host $host"
}

# get the port number (default's are set in d_opt)
p_opt () {
    port="$OPTARG"
    echo "Using port $port"
}

# get user's option to remove the database/index before re-inserting/indexing
r_opt () {
    remove=true
}


# this exectues the above functions if the corresponding argument is given
# put a leading colon at the beginning to turn on silent error processing
options='hlmf:d:D:c:i:t:H:p:s:re'
while getopts "$options" option
do
    case "$option" in
        h  ) h_opt; exit;; # HELP/documentation
        l  ) l_opt;; # data source is LOCAL (no arugment needed)
        m  ) m_opt;; # MULTIPLE files (specify directory with files)
        f  ) f_opt;; # FILE (if local and only one), directory of FILES (if local and multiple), FILE to get from census (if not local)
        d  ) d_opt;; # DATABASE type
        D  ) D_opt;; # DATABASE name (MongoDB only)
        c  ) c_opt;; # COLLECTION name (MongoDB only)
        i  ) i_opt;; # INDEX name (Elasticsearch only)
        t  ) t_opt;; # document TYPE (Elasticearch only)
        I  ) I_opt;; # IP address
        p  ) p_opt;; # PORT
        s  ) s_opt;; # STATE fips code (two digit)
        r  ) r_opt;; # REMOVE database or index before reinserting records/documents
        e  ) e_opt;; # use ESBULK (Elasticsearch only)
        \? ) echo "Unknown option: -$OPTARG" >&2; exit 1;;
        :  ) echo "Arugment required for option -$OPTARG" >&2; exit 1;;
        *  ) echo "Unused option: -$OPTARG" >&2; exit 1;;
    esac
done

# get data from TIGER
wget_census_data () {
    if [ "$is_local" != true ]
    then
        cd "$script_dir"/data/shapefiles # navigate to location of package
        if [ "$census_prod" = "county" ]
        then
            wget -nc https://www2.census.gov/geo/tiger/TIGER2016/COUNTY/tl_2016_us_county.zip
            if [ ! -e "tl_2016_us_county.shp" ]
            then
                unzip tl_2016_us_county.zip
            fi
            shapefile="$script_dir"/data/shapefiles/tl_2016_us_county.shp
        elif [ "$census_prod" = "state" ]
        then
            wget -nc https://www2.census.gov/geo/tiger/TIGER2016/STATE/tl_2016_us_state.zip
            if [ ! -e "tl_2016_us_state.shp" ]
            then
                unzip tl_2016_us_state.zip
            fi
            shapefile="$script_dir"/data/shapefiles/tl_2016_us_state.shp
        elif [ "$census_prod" = "tract" ]
        then
            if [ -z "$state_fips" ]
            then
                echo "Two digit state fips code must be specified with the -S option" >&2
                exit 1
            else
                wget -nc https://www2.census.gov/geo/tiger/TIGER2016/TRACT/tl_2016_"$state_fips"_tract.zip
                if [ ! -e "tl_2016_${state_fips}_tract.shp" ]
                then
                    unzip tl_2016_"$state_fips"_tract.zip
                fi
            fi
            shapefile="$script_dir"/data/shapefiles/tl_2016_"$state_fips"_tract.shp
        fi
    fi
}

# convert shapefile to .geojson
shp2geojson () {
    cd "$script_dir"/data/geojson
    if [ -a "$1" ] # check if shapefile exists
    then
        geojson="$(basename "$1" .shp).geojson" # use basename of file to create .geojson name
        echo "Converting shapefile to .geojson"
        ogr2ogr -f GeoJSON "$geojson" "$1" -t_srs EPSG:4326
        #http://spatialreference.org/ref/epsg/4326/ not working anymore?
    else
        echo "Shapefile does not exist" >&2
        exit 1
    fi
}

# if multiple files are to be indexed/inserted
shp2geojson_mult () {
    if [ "$multiple_files" = true ]
    then
        for i in "$shapefile_dir"/*shp
        do
            shp2geojson $i
        done
    else
        shp2geojson "$shapefile"
    fi
}

# format geojson for elasticsearch and mongodb
format_geojson () {
    cd "$script_dir"/data/geojson
    if [ -a "$1" ] # check if geojson exists
    then
        echo "Formatting geojson for database"
        # use basename of geojson to create es formatted .geojson name
        geojson_fmt="$(basename "$1" .geojson)"_fmt.geojson
        # TODO remove this and just use one file?
        cp "$1" "$geojson_fmt"

        ## delete the first four lines
        sed -i '1,4d' "$geojson_fmt"

        ## delete last character if it's a comma; we don't want a json array
        sed -i 's/,$//' "$geojson_fmt"

        # remove the last two lines
        sed -i '$d' "$geojson_fmt"
        sed -i '$d' "$geojson_fmt"

        ## steps below not necessary if using mongodb or esbulk is installed
        if [ "$use_esbulk" = "false" ] && [ "$db_type" = "elasticsearch" ]
        then
            # satisfy requirements of the bulk api
            sed -i 's/^/{ "index" : { "_index" : \"'"$index_name"'\", "_type" : \"'"$doc_type"'\" } }\n/' "$geojson_fmt" # insert index info on each line

            # add newline to end of file to satisfy bulk api
            sed -i '$a\' "$geojson_fmt"
        fi
    else
        echo "Geojson conversion for elasticsearch failed"
    fi
}

format_geojson_mult () {
    if [ "$multiple_files" = true ]
    then
        for i in "$shapefile_dir"/*shp
        do
            ## need to get the name of each geojson based on the shapefile names
            geojson="$(basename "$i" .shp).geojson" 
            format_geojson $geojson # 
        done
    else
        format_geojson "$geojson"
    fi
}

## remove database before inserting records
remove_database () {
    if [ "$remove" = "true" ] && [ "$db_type" = "elasticsearch" ]
    then
        ## check if index exists
        index_exists="$(curl -I "$host":"$port"/"$index_name")"
        if [[ "$index_exists" =~ .*"200"* ]]
        then
            echo "Removing index $index_name"
            curl -XDELETE "$host":"$port"/"$index_name"
        elif [[ "$index_exists" =~ .*"404"* ]]
        then
            echo "Index does not exist; nothing to remove"
        else
            echo "Connection refused. Is elasticsearch running?" >&2
            exit 1
        fi
    elif [ "$remove" = "true" ] && [ "$db_type" = "mongodb" ]
    then
        ## this does not throw an error if database doesn't exist
        mongo "$db_name" --eval "db.dropDatabase()"
        echo "Database deleted"
    fi
}

## in order to use a spatial index, you must first define the geometry field as
## type "geo_shape" (points must be listed in this way as well if they are to
## use spatial functions). The trick with this is that you cannot redefine field
## mappings after the index is created. The strategy here is to input one
## record, get the mapping and save it in a file, delete the index, modify the
## mapping slightly, create an index with no records, and input the field
## mappings. Then you can index records. Whew! Good thing this function exists!

input_mapping () {
    if [ "$db_type" = "elasticsearch" ]
    then
        ## navigate to location of mapping
        cd "$script_dir"/data/mappings

        ## first line of geojson will be different if esbulk is not installed
        if [ "$use_esbulk" = "true" ]
        then
            # get first line of geojson
            head -1 "$script_dir"/data/geojson/"$geojson_fmt" | cat mapping-template.json - > index-sample.json
        else
            # get second line of geojson (first line contains request)
            sed '2q;d' "$script_dir"/data/geojson/"$geojson_fmt" | cat mapping-template.json - > index-sample.json
        fi

        curl -s -XPOST "$host":"$port"/_bulk --data-binary @index-sample.json

        ## get mapping with curl; it will not be pretty
        curl -XGET "$host":"$port"/mapping_sample__/_mapping > mapping-sample.json

        ## make mapping pretty
        python -m json.tool mapping-sample.json > mapping-pretty.json

        ## delete the index; we will make it cooler (e.g. have a spatial index)
        curl -XDELETE "$host":"$port"/mapping_sample__

        ## delete third and fourth lines
        sed -i '3,4d' mapping-pretty.json
        ## insert proper info for geo_index
        sed -i '/geometry": {/a "type"\: "geo_shape"\n},' mapping-pretty.json
        ## delete lines 7-21
        sed -i '7,21d' mapping-pretty.json
        ## delete last two lines
        sed -i '$d' mapping-pretty.json
        sed -i '$d' mapping-pretty.json
        ## replace psuedo index name with actual
        sed -i 's/mapping_sample__/'"$doc_type"'/' mapping-pretty.json
        ## create trivial index (with no documents)
        curl -XPUT "$host":"$port"/"$index_name"
        ## input mapping
        curl -XPUT "$host":"$port"/"$index_name"/_mapping/"$doc_type" --data @mapping-pretty.json
    fi
}

insert_records () {
    cd "$script_dir"/data/geojson

    if [ "$db_type" = "elasticsearch" ]
    then
        printf "\nIndexing documents into elasticsearch"
        if [ "$use_esbulk" = "true" ]
        then
            #TODO allow for remote connection
            #esbulk -index "$index_name" -port "$port" -type "$doc_type" "$1" -verbose
            esbulk -index "$index_name" -host "$host" -port "$port" -type "$doc_type" "$1" -verbose
        else
            #specifying max doc size in elasticsearch.yml does not help
            num_lines=$(wc -l < "$1")
            # split files up if there are too many (3k line limit in Elasticsearch?)
            if [ "$num_lines" -gt 3000 ]
            then
                printf "\nToo many records in one file; splitting into chunks"
                # put the split files in a different directory
                cd split_dir
                split -l 2000 ../"$1" split_file # split_file will be prefix
                for i in split_file*
                do
                    printf "\nIndexing chunk $i"
                    curl -s XPOST "$host":"$port"/_bulk --data-binary @"$i"
                done
                rm split_file* # cleanup
            else
                curl -s XPOST "$host":"$port"/_bulk --data-binary @"$1"
            fi
        fi
    elif [ "$db_type" = "mongodb" ]
    then
        echo "Inserting records into MongoDB"
        mongoimport --db "$db_name" --collection "$collection_name" --file "$1" --host "$host":"$port"
    fi
}

## check if multiple_files option is selected, then use insert_records function
insert_records_mult () {
    if [ "$multiple_files" = true ]
    then
        for i in "$shapefile_dir"/*shp
        do
            ## need to get the name of each geojson based on the shapefile names
            geojson="$(basename "$i" .shp).geojson"
            ## need to get the name of each geojson_fmt based on geojson name
            geojson_fmt="$(basename "$geojson" .geojson)"_fmt.geojson
            insert_records "$geojson_fmt"
        done
    else
        insert_records "$geojson_fmt"
    fi
}

wget_census_data
shp2geojson_mult
format_geojson_mult
remove_database
input_mapping
insert_records_mult

printf "\nComplete"
